import type { OutputFor } from '../_prefabs/_prefabs'

import { cnet_preprocessor_ui_common, cnet_ui_common } from './cnet_ui_common'

// üÖøÔ∏è OPEN POSE FORM ===================================================
export type UI_subform_OpenPose = X.XGroup<{
   preprocessor: UI_subform_OpenPose_Preprocessor
   cnet_model_name: X.XEnum<Comfy.Enums['ControlNetLoader.control_net_name']>
   strength: X.XNumber
   advanced: X.XGroup<{
      startAtStepPercent: X.XNumber
      endAtStepPercent: X.XNumber
      crop: X.XEnum<Comfy.Enums['LatentUpscale.crop']>
      upscale_method: X.XEnum<Comfy.Enums['ImageScale.upscale_method']>
   }>
}>

export function ui_subform_OpenPose(): UI_subform_OpenPose {
   const form: X.Builder = getCurrentForm()
   return form
      .fields(
         {
            ...cnet_ui_common(form),
            preprocessor: ui_subform_OpenPose_Preprocessor(),
            cnet_model_name: form.enum['ControlNetLoader.input.control_net_name']({
               label: 'Model',
               // @ts-ignore
               default: 't2iadapter_openpose_sd14v1.pth',
               filter: (name) => name.toString().includes('pose'),
               extraDefaults: ['t2iadapter_openpose_sd14v1.pth', 'control_v11p_sd15_openpose.pth'],
               recommandedModels: {
                  knownModel: [
                     'T2I-Adapter (openpose)',
                     'ControlNet-v1-1 (openpose; fp16)',
                     'SDXL-controlnet: OpenPose (v2)',
                  ],
               },
            }),
         },
         { label: 'Pose' },
      )
      .addRequirements([{ type: 'customNodesByTitle', title: 'ComfyUI-Advanced-ControlNet' }])
}

// ================================================================================================

type UI_subform_OpenPose_Preprocessor = X.XChoice<{
   None: X.XEmpty
   DWPose: X.XGroup<{
      detect_body: X.XBool
      detect_face: X.XBool
      detect_hand: X.XBool
      bbox_detector: X.XEnum<Enum_DWPreprocessor$_Provider$_for$_SEGS_$3$3Inspire_bbox_detector>
      pose_estimator: X.XEnum<Enum_DWPreprocessor$_Provider$_for$_SEGS_$3$3Inspire_pose_estimator>
      saveProcessedImage: X.XBool
   }>
   OpenPose: X.XGroup<{
      detect_body: X.XBool
      detect_face: X.XBool
      detect_hand: X.XBool
      saveProcessedImage: X.XBool
   }>
}>

function ui_subform_OpenPose_Preprocessor(): UI_subform_OpenPose_Preprocessor {
   const form: X.Builder = getCurrentForm()
   return form.choice(
      {
         None: form.empty(),
         DWPose: form.group({
            label: 'Settings',
            startCollapsed: true,
            items: {
               ...cnet_preprocessor_ui_common(form),
               detect_body: form.bool({ default: true }),
               detect_face: form.bool({ default: true }),
               detect_hand: form.bool({ default: true }),
               bbox_detector: form.enum['DWPreprocessor.input.bbox_detector']({
                  label: 'Model',
                  default: 'yolox_l.onnx',
               }),
               pose_estimator: form.enum['DWPreprocessor.input.pose_estimator']({
                  label: 'Model',
                  default: 'dw-ll_ucoco_384.onnx',
               }),
            },
         }),
         OpenPose: form.group({
            label: 'Settings',
            startCollapsed: true,
            items: {
               ...cnet_preprocessor_ui_common(form),
               detect_body: form.bool({ default: true }),
               detect_face: form.bool({ default: true }),
               detect_hand: form.bool({ default: true }),
            },
         }),
         // TODO: Add support for auto-modifying the resolution based on other form selections
         // TODO: Add support for auto-cropping
      },
      { label: 'Preprocessor', startCollapsed: true, appearance: 'tab' },
   )
}

// üÖøÔ∏è OPEN POSE RUN ===================================================
export const run_cnet_openPose = (
   openPose: OutputFor<typeof ui_subform_OpenPose>,
   image: _IMAGE,
   resolution: number, // 512 | 768 | 1024 = 512,
): {
   image: _IMAGE
   cnet_name: Enum_ControlNetLoader_control_net_name
} => {
   const run = getCurrentRun()
   const graph = run.nodes
   const cnet_name = openPose.cnet_model_name

   let returnImage = image
   //crop the image to the right size
   //todo: make these editable
   // image = graph.ImageScale({
   //     image,
   //     width: cnet_args.width ?? 512,
   //     height: cnet_args.height ?? 512,
   //     upscale_method: openPose.advanced?.upscale_method ?? 'lanczos',
   //     crop: openPose.advanced?.crop ?? 'center',
   // })._IMAGE

   if (openPose.preprocessor) {
      var opPP = openPose.preprocessor
      if (opPP.OpenPose) {
         returnImage = graph.OpenposePreprocessor({
            image: image,
            detect_body: opPP.OpenPose.detect_body ? 'enable' : 'disable',
            detect_face: opPP.OpenPose.detect_face ? 'enable' : 'disable',
            detect_hand: opPP.OpenPose.detect_hand ? 'enable' : 'disable',
            resolution: resolution,
         })._IMAGE
         if (opPP.OpenPose.saveProcessedImage)
            graph.SaveImage({ images: returnImage, filename_prefix: 'cnet\\pose\\' })
         else graph.PreviewImage({ images: returnImage })
      } else if (opPP.DWPose) {
         returnImage = graph.DWPreprocessor({
            image: image,
            detect_body: opPP.DWPose.detect_body ? 'enable' : 'disable',
            detect_face: opPP.DWPose.detect_face ? 'enable' : 'disable',
            detect_hand: opPP.DWPose.detect_hand ? 'enable' : 'disable',
            resolution: resolution,
            bbox_detector: opPP.DWPose.bbox_detector,
            pose_estimator: opPP.DWPose.pose_estimator,
         })._IMAGE
         if (opPP.DWPose.saveProcessedImage)
            graph.SaveImage({ images: returnImage, filename_prefix: 'cnet\\pose\\' })
         else graph.PreviewImage({ images: returnImage })
      }
   }

   return { cnet_name, image: returnImage }
}
